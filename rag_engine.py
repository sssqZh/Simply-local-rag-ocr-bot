"""
RAG (Retrieval-Augmented Generation) å¼•æ“æ¨¡å— - ä¼˜åŒ–ç‰ˆ
ä¿®æ”¹è®°å½•ï¼š
1. å¼ºåˆ¶å°† Embedding æ¨¡å‹æŒ‡å®šä¸º 'bge-m3' (è§£å†³ä¸­æ–‡æ£€ç´¢é—®é¢˜)ã€‚
2. æ¸…ç†äº†é‡å¤çš„ API Key åˆå§‹åŒ–é€»è¾‘ã€‚
3. å¢åŠ äº†è°ƒè¯•æ‰“å°ï¼Œæ–¹ä¾¿æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„æ¨¡å‹ã€‚
"""
import fitz  # PyMuPDF
from rapidocr_onnxruntime import RapidOCR
import os
import re
from typing import List, Optional, Iterator, Dict, Any
from pathlib import Path
from io import BytesIO

import chromadb
from chromadb.config import Settings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from openai import OpenAI
from dotenv import load_dotenv

# --- 1. ç¯å¢ƒé…ç½®åŠ è½½ ---
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, '.env')
print(f"ğŸ“‚ æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {env_path}")
load_dotenv(dotenv_path=env_path, override=True)


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ç±»ï¼šè´Ÿè´£åŠ è½½ã€OCRè¯†åˆ«å’Œåˆ†å—"""

    def __init__(self, chunk_size: int = 800, chunk_overlap: int = 150):
        # ä¿®æ”¹å»ºè®®ï¼šä¸­æ–‡æ–‡æ¡£ chunk_size ç¨å¾®è°ƒå°ä¸€ç‚¹ï¼Œoverlap é€‚ä¸­ï¼Œæœ‰åŠ©äºæé«˜æ£€ç´¢ç²¾åº¦
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            # é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–åˆ†éš”ç¬¦ä¼˜å…ˆçº§
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )
        # åˆå§‹åŒ– OCR
        try:
            self.ocr = RapidOCR()
            self.ocr_available = True
            print("âœ… OCR æ¨¡å—åˆå§‹åŒ–æˆåŠŸ (RapidOCR)")
        except Exception as e:
            print(f"âš ï¸ OCR åˆå§‹åŒ–å¤±è´¥: {e}")
            self.ocr_available = False

    def load_pdf(self, file_content: bytes) -> str:
        text = ""
        # 1. å°è¯•ç›´æ¥æå–
        try:
            from pypdf import PdfReader
            pdf_reader = PdfReader(BytesIO(file_content))
            for page in pdf_reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
        except Exception as e:
            print(f"âš ï¸ pypdf è¯»å–å‡ºé”™: {e}ï¼Œå°è¯•åˆ‡æ¢åˆ° OCR...")

        # 2. å¦‚æœæå–å†…å®¹æå°‘ï¼Œåˆ¤å®šä¸ºæ‰«æä»¶ï¼Œå¯ç”¨ OCR
        if len(text.strip()) < 50:
            if self.ocr_available:
                print("ğŸ” æ£€æµ‹åˆ°æ‰«æç‰ˆ PDFï¼Œæ­£åœ¨è¿›è¡Œ OCR è¯†åˆ« (é€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒ)...")
                text = self._ocr_pdf(file_content)
            else:
                text = "æ— æ³•æå–æ–‡æœ¬ï¼Œä¸” OCR æ¨¡å—æœªå¯ç”¨ã€‚"
        
        return text

    def _ocr_pdf(self, file_content: bytes) -> str:
        ocr_text = ""
        try:
            with fitz.open(stream=file_content, filetype="pdf") as doc:
                total_pages = len(doc)
                for i, page in enumerate(doc):
                    pix = page.get_pixmap(dpi=150) # 150 dpi å…¼é¡¾é€Ÿåº¦
                    img_bytes = pix.tobytes("png")
                    result, _ = self.ocr(img_bytes)
                    if result:
                        page_content = "\n".join([line[1] for line in result])
                        ocr_text += page_content + "\n"
                    if (i + 1) % 5 == 0:
                        print(f"   -> OCR è¿›åº¦: {i+1}/{total_pages} é¡µ...")
        except Exception as e:
            print(f"âŒ OCR å‡ºé”™: {e}")
            return ""
        return ocr_text

    def load_markdown(self, file_content: bytes) -> str:
        try:
            return file_content.decode('utf-8')
        except UnicodeDecodeError:
            return file_content.decode('gbk', errors='ignore')

    def load_txt(self, file_content: bytes) -> str:
        try:
            return file_content.decode('utf-8')
        except UnicodeDecodeError:
            return file_content.decode('gbk', errors='ignore')

    def process_file(self, file_content: bytes, filename: str) -> List[Document]:
        file_ext = Path(filename).suffix.lower()
        if file_ext == '.pdf':
            text = self.load_pdf(file_content)
        elif file_ext in ['.md', '.markdown']:
            text = self.load_markdown(file_content)
        elif file_ext in ['.txt', '.text']:
            text = self.load_txt(file_content)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")

        text = self._clean_text(text)
        if not text.strip():
            return []

        doc = Document(
            page_content=text,
            metadata={"source": filename, "file_type": file_ext}
        )
        return self.text_splitter.split_documents([doc])

    def _clean_text(self, text: str) -> str:
        # ç®€å•çš„æ¸…æ´—ï¼Œä¿ç•™ä¸­æ–‡æ ‡ç‚¹
        text = re.sub(r'\s+', ' ', text)
        return text.strip()


class VectorStore:
    """å‘é‡å­˜å‚¨ç±»ï¼šç®¡ç† ChromaDB"""

    def __init__(self, db_path: str, collection_name: str):
        self.db_path = db_path
        self.collection_name = collection_name

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶ä½¿ç”¨ bge-m3 æ¨¡å‹ ---
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        
        # âš ï¸ å¼ºåˆ¶æŒ‡å®šä¸º bge-m3ï¼Œè¿™æ˜¯è§£å†³â€œæ­»é”â€æœä¸åˆ°çš„å…³é”®
        # å¦‚æœç¯å¢ƒå˜é‡æ²¡è®¾ï¼Œå°±é»˜è®¤ bge-m3
        ollama_model = os.getenv("OLLAMA_MODEL", "bge-m3") 

        print(f"ğŸ§  æ­£åœ¨åˆå§‹åŒ– Embedding æ¨¡å‹: {ollama_model} (åœ°å€: {ollama_base_url})")

        try:
            self.embeddings = OllamaEmbeddings(
                base_url=ollama_base_url,
                model=ollama_model
            )
        except Exception as e:
            print(f"âŒ Embedding æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e

        # åˆå§‹åŒ– ChromaDB
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )

        try:
            self.collection = self.client.get_collection(name=collection_name)
        except:
            self.collection = self.client.create_collection(name=collection_name)

        self.vectorstore = Chroma(
            client=self.client,
            collection_name=collection_name,
            embedding_function=self.embeddings
        )

    def add_documents(self, documents: List[Document]) -> List[str]:
        return self.vectorstore.add_documents(documents)

    def similarity_search(self, query: str, k: int = 4) -> List[Document]:
        return self.vectorstore.similarity_search(query, k=k)

    def delete_collection(self):
        try:
            self.client.delete_collection(name=self.collection_name)
            # é‡æ–°åˆ›å»º
            self.collection = self.client.create_collection(name=self.collection_name)
            # é‡æ–°ç»‘å®š LangChain æ¥å£
            self.vectorstore = Chroma(
                client=self.client,
                collection_name=self.collection_name,
                embedding_function=self.embeddings
            )
            print("ğŸ—‘ï¸ çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"åˆ é™¤é›†åˆå¤±è´¥: {str(e)}")


class RAGEngine:
    """RAG å¼•æ“ä¸»ç±»"""

    def __init__(self):
        # 1. å‘é‡åº“é…ç½®
        self.db_path = os.getenv("CHROMA_DB_PATH", "./chroma_db")
        self.collection_name = os.getenv("CHROMA_COLLECTION_NAME", "knowledge_base")
        chunk_size = int(os.getenv("MAX_CHUNK_SIZE", "800"))
        chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "150"))

        # 2. åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å’Œå‘é‡åº“
        self.doc_processor = DocumentProcessor(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        self.vectorstore = VectorStore(db_path=self.db_path, collection_name=self.collection_name)

        # 3. åˆå§‹åŒ– DeepSeek API (æ¸…ç†äº†é‡å¤ä»£ç )
        api_key = os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

        if not api_key:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ° DEEPSEEK_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
            raise ValueError("API Key Missing")

        print(f"ğŸ¤– æ­£åœ¨è¿æ¥ DeepSeek API...")
        self.llm_client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )

        self.conversation_history: List[Dict[str, str]] = []

    def add_document(self, file_content: bytes, filename: str) -> Dict[str, Any]:
        """æ·»åŠ æ–‡æ¡£"""
        try:
            documents = self.doc_processor.process_file(file_content, filename)
            if not documents:
                return {"success": False, "message": "æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯†åˆ«", "chunks_count": 0}
            
            doc_ids = self.vectorstore.add_documents(documents)
            return {
                "success": True,
                "message": f"æˆåŠŸæ·»åŠ : {filename}",
                "chunks_count": len(documents),
                "doc_ids": doc_ids
            }
        except Exception as e:
            return {"success": False, "message": f"æ·»åŠ å¤±è´¥: {str(e)}", "chunks_count": 0}

    def _build_prompt(self, query: str, context_docs: List[Document]) -> str:
        """æ„å»ºæç¤ºè¯"""
        if not context_docs:
            context = "ï¼ˆæ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³èƒŒæ™¯ä¿¡æ¯ï¼‰"
        else:
            context = "\n\n".join([f"[å‚è€ƒç‰‡æ®µ {i+1}]\n{doc.page_content}" for i, doc in enumerate(context_docs)])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥ç¨‹çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·åŸºäºä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{context}

ã€é—®é¢˜ã€‘ï¼š{query}

è¦æ±‚ï¼š
1. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æœ‰ç­”æ¡ˆï¼Œè¯·è¯¦ç»†å¼•ç”¨èµ„æ–™å›ç­”ã€‚
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸é—®é¢˜æ— å…³ï¼Œè¯·å¿½ç•¥èµ„æ–™ï¼Œåˆ©ç”¨ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”ã€‚
3. å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œé€‚åˆå·¥ç¨‹ç®¡ç†äººå‘˜é˜…è¯»ã€‚
"""
        return prompt

    def query(self, query: str, stream: bool = False) -> Iterator[str]:
        """æŸ¥è¯¢å…¥å£"""
        # 1. æ£€ç´¢ (Top-K è®¾ä¸º 4 æˆ– 5ï¼Œç»™ DeepSeek æ›´å¤šä¸Šä¸‹æ–‡)
        try:
            print(f"ğŸ” æ­£åœ¨æ£€ç´¢: {query}")
            context_docs = self.vectorstore.similarity_search(query, k=5)
            # è°ƒè¯•ï¼šæ‰“å°æ£€ç´¢åˆ°çš„ç‰‡æ®µå‰50ä¸ªå­—ï¼Œçœ‹çœ‹å‡†ä¸å‡†
            for i, doc in enumerate(context_docs):
                print(f"   [ç‰‡æ®µ{i+1}] {doc.page_content[:50].replace(chr(10), ' ')}...")
        except Exception as e:
            print(f"âš ï¸ æ£€ç´¢å‡ºé”™ (å¯èƒ½æ˜¯åº“ä¸ºç©º): {e}")
            context_docs = []

        # 2. æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(query, context_docs)

        # 3. æ¶ˆæ¯å†å² (System Prompt + User Prompt)
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]

        # 4. è°ƒç”¨ DeepSeek
        try:
            response = self.llm_client.chat.completions.create(
                model="deepseek-chat",
                messages=messages,
                stream=stream,
                temperature=0.3 # é™ä½æ¸©åº¦ï¼Œè®©å›ç­”æ›´ä¸¥è°¨
            )
            
            if stream:
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
            else:
                yield response.choices[0].message.content

        except Exception as e:
            yield f"API è°ƒç”¨å‡ºé”™: {str(e)}"

    def clear_knowledge_base(self):
        self.vectorstore.delete_collection()
        self.conversation_history.clear()

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # å°è¯•è·å–çœŸå®çš„ chunk æ•°é‡
            count = self.vectorstore.collection.count()
        except:
            count = 0
            
        # å¿…é¡»è¿”å› collection_name å’Œ db_pathï¼Œé˜²æ­¢ app.py æŠ¥é”™
        return {
            "total_chunks": count,
            "collection_name": self.collection_name, 
            "db_path": self.db_path,
            "model": "bge-m3 + deepseek-chat"
        }