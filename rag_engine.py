"""
RAG (Retrieval-Augmented Generation) å¼•æ“æ¨¡å—

è¯¥æ¨¡å—å®ç°äº†å®Œæ•´çš„ RAG ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æ¡£åŠ è½½å’Œåˆ†å—
- å‘é‡åŒ–å­˜å‚¨ï¼ˆChromaDBï¼‰
- æ£€ç´¢å¢å¼ºç”Ÿæˆ
- æµå¼å“åº”æ”¯æŒ
"""
import fitz  # PyMuPDF
from rapidocr_onnxruntime import RapidOCR
import os
import re
from typing import List, Optional, Iterator, Dict, Any
from pathlib import Path
from io import BytesIO

import chromadb
from chromadb.config import Settings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from openai import OpenAI
from dotenv import load_dotenv
import os

# åŸæœ‰çš„ import ...
from io import BytesIO
import chromadb


#1. è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
# 2. æ‹¼æ¥å‡º .env çš„ç»å¯¹è·¯å¾„
env_path = os.path.join(current_dir, '.env')

# 3. æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆè®©ä½ çœ‹ç€æ”¾å¿ƒï¼‰
print(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {env_path}")

# 4. å¼ºåˆ¶åŠ è½½
load_dotenv(dotenv_path=env_path, override=True)


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ç±»ï¼Œè´Ÿè´£åŠ è½½å’Œé¢„å¤„ç†å„ç§æ ¼å¼çš„æ–‡æ¡£"""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        # åˆå§‹åŒ– OCR å®ä¾‹ (åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹)
        try:
            self.ocr = RapidOCR()
            self.ocr_available = True
            print("âœ… OCR æ¨¡å—åˆå§‹åŒ–æˆåŠŸ (RapidOCR)")
        except Exception as e:
            print(f"âš ï¸ OCR åˆå§‹åŒ–å¤±è´¥: {e}")
            self.ocr_available = False

    def load_pdf(self, file_content: bytes) -> str:
        """
        åŠ è½½ PDF æ–‡ä»¶å†…å®¹ (æ”¯æŒæ‰«æä»¶ OCR)
        """
        text = ""
        
        # 1. å°è¯•ä½¿ç”¨ pypdf æå–æ–‡æœ¬ (é€Ÿåº¦å¿«ï¼Œé’ˆå¯¹éæ‰«æä»¶)
        try:
            from pypdf import PdfReader
            pdf_reader = PdfReader(BytesIO(file_content))
            for page in pdf_reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
        except Exception as e:
            print(f"âš ï¸ pypdf è¯»å–å‡ºé”™: {e}ï¼Œå°è¯•åˆ‡æ¢åˆ° OCR...")

        # 2. åˆ¤æ–­æå–ç»“æœã€‚å¦‚æœå†…å®¹ä¸ºç©ºæˆ–æå°‘(å°‘äº50ä¸ªå­—)ï¼Œåˆ¤å®šä¸ºæ‰«æä»¶ï¼Œå¯ç”¨ OCR
        if len(text.strip()) < 50:
            if self.ocr_available:
                print("ğŸ” æ£€æµ‹åˆ°æ‰«æç‰ˆ PDF æˆ–æ–‡æœ¬æå°‘ï¼Œæ­£åœ¨è¿›è¡Œ OCR è¯†åˆ« (é€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
                text = self._ocr_pdf(file_content)
            else:
                text = "æ— æ³•æå–æ–‡æœ¬ï¼Œä¸” OCR æ¨¡å—æœªå¯ç”¨ã€‚"
        
        return text

    def _ocr_pdf(self, file_content: bytes) -> str:
        """
        ä½¿ç”¨ PyMuPDF + RapidOCR è¿›è¡Œè¯†åˆ«
        """
        ocr_text = ""
        try:
            # ä½¿ç”¨ fitz (PyMuPDF) æ‰“å¼€ PDF
            with fitz.open(stream=file_content, filetype="pdf") as doc:
                total_pages = len(doc)
                for i, page in enumerate(doc):
                    # å°†é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡ (dpi=150 å…¼é¡¾é€Ÿåº¦å’Œç²¾åº¦)
                    pix = page.get_pixmap(dpi=150)
                    img_bytes = pix.tobytes("png")
                    
                    # è°ƒç”¨ RapidOCR è¯†åˆ«
                    result, _ = self.ocr(img_bytes)
                    
                    if result:
                        # result æ ¼å¼: [[box, text, score], ...]
                        page_content = "\n".join([line[1] for line in result])
                        ocr_text += page_content + "\n"
                    
                    # æ‰“å°è¿›åº¦ (å› ä¸º OCR æ¯”è¾ƒæ…¢)
                    print(f"   -> æ­£åœ¨è¯†åˆ«ç¬¬ {i+1}/{total_pages} é¡µ...")
                    
        except Exception as e:
            print(f"âŒ OCR è¯†åˆ«è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return ""
            
        return ocr_text

    def load_markdown(self, file_content: bytes) -> str:
        # ... (ä¿æŒä¸å˜) ...
        try:
            return file_content.decode('utf-8')
        except UnicodeDecodeError:
            return file_content.decode('gbk', errors='ignore')

    def load_txt(self, file_content: bytes) -> str:
         # ... (ä¿æŒä¸å˜) ...
        try:
            return file_content.decode('utf-8')
        except UnicodeDecodeError:
            return file_content.decode('gbk', errors='ignore')

    def process_file(self, file_content: bytes, filename: str) -> List[Document]:
        # ... (ä¿æŒä¸å˜ï¼Œä½†ä¸ºäº†ç¡®ä¿å®‰å…¨ï¼Œæˆ‘æŠŠä½ çš„åŸå§‹é€»è¾‘å¤åˆ¶åœ¨è¿™é‡Œ) ...
        file_ext = Path(filename).suffix.lower()

        if file_ext == '.pdf':
            text = self.load_pdf(file_content)
        elif file_ext in ['.md', '.markdown']:
            text = self.load_markdown(file_content)
        elif file_ext in ['.txt', '.text']:
            text = self.load_txt(file_content)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")

        text = self._clean_text(text)
        
        # å†æ¬¡æ£€æŸ¥ï¼šå¦‚æœç»è¿‡ OCR è¿˜æ˜¯ç©ºçš„
        if not text.strip():
            print(f"âš ï¸ æ–‡ä»¶ {filename} å¤„ç†åå†…å®¹ä¾ç„¶ä¸ºç©ºã€‚")
            return []

        doc = Document(
            page_content=text,
            metadata={"source": filename, "file_type": file_ext}
        )

        chunks = self.text_splitter.split_documents([doc])
        return chunks

    def _clean_text(self, text: str) -> str:
        # ... (ä¿æŒä¸å˜) ...
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''ï¼ˆï¼‰ã€ã€‘]', ' ', text)
        return text.strip()

class VectorStore:
    """å‘é‡å­˜å‚¨ç±»ï¼Œè´Ÿè´£ç®¡ç† ChromaDB å‘é‡æ•°æ®åº“"""

    def __init__(self, db_path: str, collection_name: str):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨

        Args:
            db_path: ChromaDB æ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        self.db_path = db_path
        self.collection_name = collection_name

        # åˆå§‹åŒ– Ollama Embeddings
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        ollama_model = os.getenv("OLLAMA_MODEL", "nomic-embed-text")

        self.embeddings = OllamaEmbeddings(
            base_url=ollama_base_url,
            model=ollama_model
        )

        # åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )

        # è·å–æˆ–åˆ›å»ºé›†åˆ
        try:
            self.collection = self.client.get_collection(name=collection_name)
        except:
            self.collection = self.client.create_collection(name=collection_name)

        # åˆå§‹åŒ– LangChain Chroma
        self.vectorstore = Chroma(
            client=self.client,
            collection_name=collection_name,
            embedding_function=self.embeddings
        )

    def add_documents(self, documents: List[Document]) -> List[str]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            æ·»åŠ çš„æ–‡æ¡£ ID åˆ—è¡¨
        """
        return self.vectorstore.add_documents(documents)

    def similarity_search(self, query: str, k: int = 4) -> List[Document]:
        """
        ç›¸ä¼¼åº¦æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡

        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        return self.vectorstore.similarity_search(query, k=k)

    def similarity_search_with_score(self, query: str, k: int = 4) -> List[tuple]:
        """
        å¸¦åˆ†æ•°çš„ç›¸ä¼¼åº¦æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡

        Returns:
            (æ–‡æ¡£, åˆ†æ•°) å…ƒç»„åˆ—è¡¨
        """
        return self.vectorstore.similarity_search_with_score(query, k=k)

    def delete_collection(self):
        """åˆ é™¤é›†åˆï¼ˆæ¸…ç©ºæ•°æ®åº“ï¼‰"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self.client.create_collection(name=self.collection_name)
            self.vectorstore = Chroma(
                client=self.client,
                collection_name=self.collection_name,
                embedding_function=self.embeddings
            )
        except Exception as e:
            print(f"åˆ é™¤é›†åˆå¤±è´¥: {str(e)}")


class RAGEngine:
    """RAG å¼•æ“ä¸»ç±»ï¼Œæ•´åˆæ–‡æ¡£å¤„ç†ã€å‘é‡å­˜å‚¨å’Œ LLM è°ƒç”¨"""

    def __init__(self):
        """åˆå§‹åŒ– RAG å¼•æ“"""
        # åŠ è½½é…ç½®
        self.db_path = os.getenv("CHROMA_DB_PATH", "./chroma_db")
        self.collection_name = os.getenv("CHROMA_COLLECTION_NAME", "knowledge_base")
        chunk_size = int(os.getenv("MAX_CHUNK_SIZE", "1000"))
        chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "200"))

        # åˆå§‹åŒ–ç»„ä»¶
        self.doc_processor = DocumentProcessor(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.vectorstore = VectorStore(
            db_path=self.db_path,
            collection_name=self.collection_name
        )

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºè°ƒç”¨ DeepSeek APIï¼‰
        api_key = os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

         # ç›´æ¥å¡«å…¥ä½ çš„ Key (æ³¨æ„ä¿ç•™å¼•å·)
        #api_key = "" 
        #base_url = "https://api.deepseek.com"

        if not api_key:
            raise ValueError("æœªæ‰¾åˆ° DEEPSEEK_API_KEY")

        self.llm_client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )



        if not api_key:
            raise ValueError("æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")

        self.llm_client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )

        # å¯¹è¯å†å²
        self.conversation_history: List[Dict[str, str]] = []

    def add_document(self, file_content: bytes, filename: str) -> Dict[str, Any]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“

        Args:
            file_content: æ–‡ä»¶å­—èŠ‚å†…å®¹
            filename: æ–‡ä»¶å

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # å¤„ç†æ–‡æ¡£
            documents = self.doc_processor.process_file(file_content, filename)

            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            doc_ids = self.vectorstore.add_documents(documents)

            return {
                "success": True,
                "message": f"æˆåŠŸæ·»åŠ æ–‡æ¡£: {filename}",
                "chunks_count": len(documents),
                "doc_ids": doc_ids
            }
        except Exception as e:
            return {
                "success": False,
                "message": f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}",
                "chunks_count": 0,
                "doc_ids": []
            }

    def _build_prompt(self, query: str, context_docs: List[Document]) -> str:
        """
        æ„å»º RAG æç¤ºè¯ (å·²ä¿®æ”¹ä¸ºæ··åˆæ¨¡å¼)
        """
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œä¸Šä¸‹æ–‡å°±æ˜¯ç©ºçš„
        if not context_docs:
            context = "ï¼ˆå½“å‰æ²¡æœ‰ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹ï¼‰"
        else:
            context = "\n\n".join([
                f"[å‚è€ƒç‰‡æ®µ {i+1}]\n{doc.page_content}"
                for i, doc in enumerate(context_docs)
            ])

        # ä¿®æ”¹æç¤ºè¯ï¼Œå…è®¸æ¨¡å‹ä½¿ç”¨é€šç”¨çŸ¥è¯†
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·å‚è€ƒä¸‹é¢çš„ã€çŸ¥è¯†åº“ç‰‡æ®µã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚

ã€çŸ¥è¯†åº“ç‰‡æ®µã€‘ï¼š
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{query}

å›ç­”è¦æ±‚ï¼š
1. å¦‚æœã€çŸ¥è¯†åº“ç‰‡æ®µã€‘ä¸­æœ‰ç­”æ¡ˆï¼Œè¯·ä¼˜å…ˆåŸºäºçŸ¥è¯†åº“å›ç­”ã€‚
2. å¦‚æœã€çŸ¥è¯†åº“ç‰‡æ®µã€‘ä¸é—®é¢˜æ— å…³æˆ–æ²¡æœ‰å†…å®¹ï¼Œè¯·å¿½ç•¥çŸ¥è¯†åº“ï¼Œç›´æ¥ä½¿ç”¨ä½ è‡ªå·±çš„é€šç”¨çŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
3. å›ç­”è¦è‡ªç„¶ã€æµç•…ã€‚
"""
        return prompt

    
    def query(self, query: str, stream: bool = False) -> Iterator[str]:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“å¹¶ç”Ÿæˆå›ç­”ï¼ˆæµå¼ï¼‰- å·²ä¿®æ”¹ä¸ºæ”¯æŒé€šç”¨é—²èŠ
        """
        # 1. å°è¯•æ£€ç´¢ç›¸å…³æ–‡æ¡£
        # æ³¨æ„ï¼šå¦‚æœæ•°æ®åº“æ˜¯ç©ºçš„ï¼Œè¿™é‡Œä¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ä¼šæŠ¥é”™
        try:
            context_docs = self.vectorstore.similarity_search(query, k=4)
        except Exception:
            # å¦‚æœæ•°æ®åº“è¿˜æ²¡åˆå§‹åŒ–æˆ–å‡ºé”™ï¼Œå°±å½“åšæ²¡æ–‡æ¡£
            context_docs = []

        # åˆ é™¤åŸæœ¬çš„ "if not context_docs: return" æ‹¦æˆªä»£ç 
        # è®©ä»£ç ç»§ç»­å¾€ä¸‹èµ°ï¼Œå»è°ƒç”¨ DeepSeek

        # 2. æ„å»ºæç¤ºè¯ (ä¼šè‡ªåŠ¨å¤„ç† context_docs ä¸ºç©ºçš„æƒ…å†µ)
        prompt = self._build_prompt(query, context_docs)

        # 3. æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
        # å¯ä»¥åœ¨ system é‡Œç¨å¾®å¼ºåŒ–ä¸€ä¸‹äººè®¾
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹ã€‚æ—¢èƒ½å›ç­”çŸ¥è¯†åº“çš„é—®é¢˜ï¼Œä¹Ÿèƒ½è¿›è¡Œæ—¥å¸¸å¯¹è¯ã€‚"},
            {"role": "user", "content": prompt}
        ]

        # 4. è°ƒç”¨ DeepSeek API (ä¿æŒåŸæ ·)
        try:
            if stream:
                response = self.llm_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=messages,
                    stream=True,
                    temperature=0.7
                )
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
            else:
                response = self.llm_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=messages,
                    stream=False,
                    temperature=0.7
                )
                yield response.choices[0].message.content

        except Exception as e:
            yield f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"

    def clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        self.vectorstore.delete_collection()
        self.conversation_history.clear()

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            count = self.vectorstore.collection.count()
            return {
                "total_chunks": count,
                "collection_name": self.collection_name,
                "db_path": self.db_path
            }
        except:
            return {
                "total_chunks": 0,
                "collection_name": self.collection_name,
                "db_path": self.db_path
            }

